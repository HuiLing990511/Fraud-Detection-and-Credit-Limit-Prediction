---
title: "Regression: Credit Limit Prediction"
author: "WQD7004 - Group 4"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Import Package

```{r package}
library(dslabs)
library(dplyr)
library(tidyr)
library(lubridate)
library(caret)
library(stats)
library(randomForest)
library(xgboost)
library(Ckmeans.1d.dp)
library(gbm)
library(e1071)
```

# 1. Data Transformation

## 1.1 Load Data
```{r loaddata}
credit_limit_data <- readRDS("CleanedDataSet/credit_limit_data.rds")
```

## 1.2 GEO CLUSTERING (K-MEANS)  
Latitude and longitude are continuous numbers which the machine learning models do not naturally understand spatial proximity and treat them as independent numbers. Hence, by using K-means to convert them into meaningful features which helps the machine learning models to capture geographical patterns.
```{r transformation1}
# ---------- GEO CLUSTERING (K-MEANS) ----------
set.seed(42)

coords <- credit_limit_data %>%
  select(latitude, longitude)

kmeans_model <- kmeans(coords, centers = 8)

credit_limit_data$geo_cluster <- NA
credit_limit_data$geo_cluster[as.numeric(rownames(coords))] <- kmeans_model$cluster
credit_limit_data$geo_cluster <- as.factor(credit_limit_data$geo_cluster)

# One-hot encode geo_cluster
geo_dummies <- model.matrix(~ geo_cluster - 1, data = credit_limit_data)
credit_limit_data <- bind_cols(
  credit_limit_data %>% select(-geo_cluster),
  as.data.frame(geo_dummies)
)

```

## 1.3 ONE-HOT ENCODING  
To convert the non-ordinal categorical variables into binary numerical format which suitable for machine learning models and avoid false ordering issue.
```{r transformation2}
card_type_dummies  <- model.matrix(~ card_type - 1, data = credit_limit_data)
card_brand_dummies <- model.matrix(~ card_brand - 1, data = credit_limit_data)

colnames(card_type_dummies)  <- gsub("^card_type", "", colnames(card_type_dummies))
colnames(card_brand_dummies) <- gsub("^card_brand", "", colnames(card_brand_dummies))

credit_limit_data <- bind_cols(
  credit_limit_data,
  as.data.frame(card_type_dummies),
  as.data.frame(card_brand_dummies)
)

head(credit_limit_data[, c("card_type", "Credit", "Debit", "Debit (Prepaid)")], 20)

head(credit_limit_data[, c("card_brand", "Amex", "Discover", "Mastercard", "Visa")], 20)

```

## 1.4 DATA CONVERSION  
- New variable "retired" is created if current age is greater than or equal to the retirement age then classified as retired (1)  
- New variable "age_group" is created by grouping the exact ages into ranges to represent different life stages  
- New variable "credit_score_category" is created by categorizing credit scores into standard risk levels  
- New variable "gender_binary" is created by converting gender from string variable into a numeric format  
- New variable "acct_tenure_years" is created by calculating the time difference between the current date and the account opening date to represent the length of time of the account has been active
```{r transformation3}
credit_limit_data <- credit_limit_data %>%
  mutate(
    retired = ifelse(current_age >= retirement_age, 1, 0)
  )

head(credit_limit_data[, c("retired", "current_age", "retirement_age")], 20)

credit_limit_data <- credit_limit_data %>%
  mutate(
    age_group = cut(
      current_age,
      breaks = c(0, 24, 40, 60, Inf),
      labels = c(1, 2, 3, 4),
      right = TRUE
    )
  )

credit_limit_data$age_group <- as.integer(as.character(credit_limit_data$age_group))

head(credit_limit_data[, c("current_age", "age_group")], 20)

credit_limit_data <- credit_limit_data %>%
  mutate(
    credit_score_category = cut(
      credit_score,
      breaks = c(0, 579, 669, 739, 799, 850),
      labels = c(1, 2, 3, 4, 5),
      right = TRUE,
      include.lowest = TRUE
    )
  )

credit_limit_data$credit_score_category <- as.integer(
  as.character(credit_limit_data$credit_score_category)
)

head(credit_limit_data[, c("credit_score", "credit_score_category")], 20)

credit_limit_data <- credit_limit_data %>%
  mutate(gender_binary = ifelse(gender == "Male", 1, 0))

head(credit_limit_data[, c("gender", "gender_binary")], 20)

REFERENCE_DATE <- as.Date("2020-03-01")

credit_limit_data <- credit_limit_data %>%
  mutate(
    acct_open_date = as.Date(acct_open_date),
    acct_tenure_years =
      as.numeric(difftime(REFERENCE_DATE, acct_open_date, units = "days")) / 365.25
  )

head(credit_limit_data[, c("acct_open_date", "acct_tenure_years")], 20)
summary(credit_limit_data$acct_tenure_years)
```

## 1.5 DROP COLUMNS  
By removing redundant or no longer needed features to ensure the dataset contains only relevant features for modeling
```{r dropcolumn}
# ---------- DROP ORIGINAL STRING / SENSITIVE COLUMNS ----------
columns_to_drop <- c(
  "gender", "acct_open_date", "card_number", "expires", "cvv",
  "has_chip", "birth_year", "birth_month", "address", "latitude",
  "longitude", "transaction_frequency", "card_type", "card_brand",
  "retirement_age", "current_age","year_pin_last_changed", "avg_errors",
  "total_errors","num_refunds", "total_refunded",
  "min_transaction_amount", "credit_score"
)

# Drop only columns that exist
columns_to_drop <- intersect(columns_to_drop, names(credit_limit_data))

credit_limit_data <- credit_limit_data %>%
  select(-all_of(columns_to_drop))

colnames(credit_limit_data)
```

# 2. Data Splitting
To split the dataset into training (80%) and testing (20%) at the client level to ensure that all transactions for a given client stay in the same set. This is done to avoid data leakage and ensures the same client won't appear in both training and testing.
```{r shuffle}
set.seed(42)
clients <- unique(credit_limit_data$client_id)
n <- length(clients)
clients <- sample(clients)

# Split indices
train_index <- 1:floor(0.8 * n)
test_index  <- (floor(0.8*n) + 1):n

# Assign clients to each set
train_clients <- clients[train_index]
test_clients  <- clients[test_index]

# Subset dataset
train_data <- credit_limit_data %>% filter(client_id %in% train_clients)
test_data  <- credit_limit_data %>% filter(client_id %in% test_clients)

# Separate features and target
x_train <- train_data %>% select(-credit_limit, -id, -client_id)
y_train <- train_data$credit_limit

x_test <- test_data %>% select(-credit_limit, -id, -client_id)
y_test <- test_data$credit_limit
```

# 3. MULTICOLLINEARITY CHECK
To check whether any highly correlated numeric features exists and removes them from both training and testing data if the correlations is greater than 0.8
```{r corr}
num_data <- credit_limit_data %>%
  select(where(is.numeric))

cor_matrix <- cor(num_data, use = "complete.obs")

cor_df <- as.data.frame(as.table(cor_matrix)) %>%
  rename(
    var1 = Var1,
    var2 = Var2,
    correlation = Freq
  ) %>%
  filter(
    abs(correlation) > 0.8,
    var1 != var2
  )

cor_df <- cor_df %>%
  rowwise() %>%
  mutate(pair = paste(sort(c(var1, var2)), collapse = "_")) %>%
  ungroup() %>%
  distinct(pair, .keep_all = TRUE) %>%
  select(-pair)

knitr::kable(cor_df, digits = 3)

x_train <- x_train %>%
  select(-per_capita_income, -total_transactions)
x_test <- x_test %>%
  select(-per_capita_income, -total_transactions)
```

# 4. MODEL TRAINING
5 regression models are trained to predict the credit limit  
- Linear Regression (LM)  
- Random Forest (RF)  
- eXtreme Gradient Boosting (XGBoost)  
- Gradient Boosting Machine (GBM)  
- Support Vector Regression (SVR)  

Evaluation Metrics  
- R-squared (R2): Measures the standard deviation of prediction errors. Lower is better  
- RMSE (Root Mean Squared Error): Average absolute difference between predictions and actual values. Lower is better  
- MAE (Mean Absolute Error): Average absolute difference between predictions and actual values. Lower is better  
```{r lm}
# ---------------- Train Linear Regression model ----------------
model_lm <- train(
  x = x_train,
  y = y_train,
  method = "lm"
)

lm_pred <- predict(model_lm, x_test)

lm_rmse <- RMSE(lm_pred, y_test)
lm_mae  <- MAE(lm_pred, y_test)
lm_r2   <- R2(lm_pred, y_test)


# ---------------- Train Random Forest model ----------------
set.seed(42)

rf_model <- randomForest(
  x = x_train,
  y = y_train,
  ntree = 500,        # n_estimators
  importance = TRUE
)

rf_pred <- predict(rf_model, x_test)


rf_rmse <- RMSE(rf_pred, y_test)
rf_mae  <- MAE(rf_pred, y_test)
rf_r2   <- R2(rf_pred, y_test)

# ------------------- Train XGBoost model -------------------

# Convert numeric data frames to matrices
x_train_matrix <- as.matrix(x_train)
x_test_matrix  <- as.matrix(x_test)

y_train_vector <- as.numeric(y_train)
y_test_vector  <- as.numeric(y_test)

dtrain <- xgb.DMatrix(data = x_train_matrix, label = y_train_vector)
dtest  <- xgb.DMatrix(data = x_test_matrix, label = y_test_vector)

set.seed(42)

xgb_params <- list(
  objective = "reg:squarederror",
  eta = 0.05,
  max_depth = 6,
  subsample = 0.8,
  colsample_bytree = 0.8
)

xgb_model <- xgb.train(
  params = xgb_params,
  data = dtrain,
  nrounds = 500,
  verbose = 0
)
xgb_pred <- predict(xgb_model, newdata = dtest)
xgb_rmse <- RMSE(xgb_pred, y_test_vector)
xgb_mae <- MAE(xgb_pred, y_test_vector)
xgb_r2 <- R2(xgb_pred, y_test_vector)

# ------------------- Train GBM model -------------------
set.seed(42)

model_gbm <- gbm(
  formula = credit_limit ~ . -id -client_id,
  data = train_data,
  distribution = "gaussian",
  n.trees = 500,
  interaction.depth = 6,
  shrinkage = 0.1,
  n.minobsinnode = 10,
  verbose = FALSE
)

gbm_pred <- predict(model_gbm, newdata = test_data, n.trees = 500)
gbm_rmse <- RMSE(gbm_pred, y_test)
gbm_mae  <- MAE(gbm_pred, y_test)
gbm_r2   <- R2(gbm_pred, y_test)

# ------------------- Train SVR model -------------------
model_svr <- svm(
  x = x_train,
  y = y_train,
  type = "eps-regression",   # SVR
  kernel = "radial",         # RBF kernel
  cost = 10,                 # C parameter
  gamma = 1 / ncol(x_train), # common default
  epsilon = 0.1
)

svr_pred <- predict(model_svr, x_test)
svr_rmse <- RMSE(svr_pred, y_test)
svr_mae  <- MAE(svr_pred, y_test)
svr_r2   <- R2(svr_pred, y_test)

# ------------------- Summary Table -------------------
results <- data.frame(
  Model = c("Linear Regression", "Random Forest", "XGBoost", "GBM", "SVR"),
  RMSE  = c(lm_rmse, rf_rmse, xgb_rmse, gbm_rmse, svr_rmse),
  MAE   = c(lm_mae, rf_mae, xgb_mae, gbm_mae, svr_mae),
  R2    = c(lm_r2, rf_r2, xgb_r2, gbm_r2, svr_r2)
)

results
```

# 5. HYPERPARAMETER TUNING
In order to improve the prediction performance, hyperparameter tuning is performed on the XGBoost (best-performing model in previous section) by testing different learning rates (eta) and depths (max_depth)
```{r tuning}
# Convert numeric data frames to matrices
x_train_matrix <- as.matrix(x_train)
x_test_matrix  <- as.matrix(x_test)

y_train_vector <- as.numeric(y_train)
y_test_vector  <- as.numeric(y_test)

dtrain <- xgb.DMatrix(data = x_train_matrix, label = y_train_vector)
dtest  <- xgb.DMatrix(data = x_test_matrix, label = y_test_vector)

# Set of 3 hyperparameter combinations
param_list <- list(
  list(eta = 0.05, max_depth = 6),
  list(eta = 0.01,  max_depth = 6),
  list(eta = 0.1, max_depth = 6),
  list(eta = 0.05, max_depth = 5),
  list(eta = 0.01,  max_depth = 5),
  list(eta = 0.1, max_depth = 5),
  list(eta = 0.05, max_depth = 4),
  list(eta = 0.01,  max_depth = 4),
  list(eta = 0.1, max_depth = 4)
)

# Store results
results <- data.frame(
  Set = character(),
  RMSE = numeric(),
  MAE  = numeric(),
  R2   = numeric()
)

models_list <- list()

# Train and evaluate each set
for (i in seq_along(param_list)) {
  params <- param_list[[i]]
  
  set.seed(42)
  xgb_params <- list(
    objective = "reg:squarederror",
    eta = params$eta,
    max_depth = params$max_depth,
    subsample = 0.8,
    colsample_bytree = 0.8
  )
  
  model <- xgb.train(
    params = xgb_params,
    data = dtrain,
    nrounds = 500,
    verbose = 0
  )
  
  pred <- predict(model, newdata = dtest)
  
  results <- rbind(
    results,
    data.frame(
      Set = paste0("Set_", i),
      RMSE = RMSE(pred, y_test_vector),
      MAE  = MAE(pred, y_test_vector),
      R2   = R2(pred, y_test_vector)
    )
  )
  
  models_list[[i]] <- model
}

# Show comparison
results
```

# 6. FEATURE IMPORTANCE
- Feature importance is extracted to show which variables contributed most to predictions  
- Additional evaluation metrics including residuals, Median Absolute Error (MedAE), and Mean Absolute Percentage Error (MAPE) are calculated  
- Prediction vs actual and residual plots are visualized for performance assessment
```{r importance}
# ------------------- Find best model -------------------
best_index <- which.min(results$RMSE)
best_model <- models_list[[best_index]]
best_set_name <- results$Set[best_index]

cat("Best hyperparameter set:", best_set_name, "\n")
cat("RMSE:", round(results$RMSE[best_index],3),
    "MAE:", round(results$MAE[best_index],3),
    "R2:", round(results$R2[best_index],3), "\n")

# ----------- Feature importance for best model -------------------
importance_matrix <- xgb.importance(
  feature_names = colnames(x_train_matrix),
  model = best_model
)

n_features <- nrow(importance_matrix)
cat("Total features:", n_features, "\n")

# View top 10 features
head(importance_matrix, 10)

# Plot top 10 features
xgb.plot.importance(importance_matrix[1:10, ])

# ------------------- Additional Results for XGBoost -------------------

# Predict using best model
best_pred <- predict(best_model, newdata = dtest)

# 1. Residuals
residuals <- y_test_vector - best_pred
residuals

# 2. Median Absolute Error (MedAE)
medae <- median(abs(residuals))
medae

# 3. Mean Absolute Percentage Error (MAPE)
mape <- mean(abs(residuals / y_test_vector)) * 100
mape

# 4. Prediction vs Actual Plot
plot(y_test_vector, best_pred,
     xlab = "Actual Credit Limit",
     ylab = "Predicted Credit Limit",
     main = paste("XGBoost (Best) Predicted vs Actual -", best_set_name))
abline(a = 0, b = 1, col = "red", lwd = 2)

# 5. Residual Plot
plot(best_pred, residuals,
     xlab = "Predicted Credit Limit",
     ylab = "Residuals",
     main = paste("XGBoost (Best) Residuals vs Predicted -", best_set_name))
abline(h = 0, col = "red", lwd = 2)

```