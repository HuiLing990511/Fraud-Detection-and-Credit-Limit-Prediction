---
title: "Model Evaluation Summary"
author: "WQD7004 - Group 4"
date: "2026-01-01"
output: 
  html_document:
    theme: cosmo
    toc: true
    toc_depth: 2
---


# Global settings: Hide code, show results only
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Load Required Packages
```{r}
library(dplyr)
library(ggplot2)
library(caret)
library(pROC)
```

# 1. Regression Analysis: Credit Limit 
Data Source: Extracted from '2.2-Regression.html' Summary Table

```{r}
regression_results <- data.frame(
  Model = c("Linear Regression", "Random Forest", "GBM", "SVR", "XGBoost (Initial)", "XGBoost (Tuned)"),
  RMSE  = c(9160.61, 8781.93, 8448.87, 9222.76, 8393.19, 8213.91),
  R2    = c(0.548, 0.591, 0.611, 0.536, 0.614, 0.633)
)
```

### 1.1 Model Performance Comparison
Calculate improvement percentage
```{r}
baseline_rmse <- regression_results$RMSE[1]
best_rmse <- min(regression_results$RMSE)
improvement <- round((baseline_rmse - best_rmse) / baseline_rmse * 100, 1)

print(regression_results)
```
Plot: RMSE Comparison

```{r}
ggplot(regression_results, aes(x = reorder(Model, RMSE), y = RMSE, fill = RMSE)) +
  geom_bar(stat = "identity", alpha = 0.8, width = 0.8) +
  coord_flip() +
  scale_fill_gradient(low = "#2ecc71", high = "#e74c3c") +  # Improved color gradient
  geom_text(aes(label = sprintf("%.0f", RMSE)), hjust = 1.1, color = "black", fontface = "bold") +
  labs(
    title = "RMSE Comparison (Lower = Better)",
    subtitle = paste0("Tuned XGBoost reduced prediction error by ", improvement, "% vs. Linear Regression"),
    x = "Model", 
    y = "Root Mean Squared Error (RMSE)"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 14, face = "bold", hjust = 0.5),
    plot.subtitle = element_text(size = 12, color = "#666", hjust = 0.5),
    axis.title = element_text(size = 11),
    legend.position = "none"  # Hide redundant legend (RMSE values labeled directly)
  )
```

### 1.2 Feature Importance (Tuned XGBoost)
Data Source: Extracted from '2.2-Regression.html' Feature Importance Plot

Manually constructing the top features based on the HTML output
```{r}
imp_data <- data.frame(
  Feature = c("Yearly Income", "Debit", "Credit", "Debit (Prepaid)", 
              "Acct Tenure", "Total Debt", "Debt/Income Ratio", "Avg Txn Amount"),
  Gain = c(0.4055, 0.1882, 0.0730, 0.0531, 
           0.0450, 0.0440, 0.0343, 0.0338)
)

ggplot(imp_data, aes(x = reorder(Feature, Gain), y = Gain)) +
  geom_bar(stat = "identity", fill = "#3498db", alpha = 0.8, width = 0.8) +
  coord_flip() +
  geom_text(aes(label = sprintf("%.4f", Gain)), hjust = 1.1, color = "black", fontface = "bold") +
  labs(
    title = "Top Feature Importance (by Information Gain)",
    subtitle = "Yearly Income is the dominant predictor for Credit Limit",
    x = "Feature", 
    y = "Information Gain"
  ) +
  theme_light() +
  theme(
    plot.title = element_text(size = 14, face = "bold", hjust = 0.5),
    plot.subtitle = element_text(size = 12, color = "#666", hjust = 0.5),
    axis.title = element_text(size = 11)
  )
```

Observation: Consistent with financial logic, Yearly Income (Gain: 0.4055) is the most critical factor in determining credit limits, followed by debit account activity.


# 2. Classification Analysis: Fraud Detection
Source: Data extracted from 3.3-classification.html.
Given the imbalanced nature of fraud datasets, we focused on ROC-AUC and F1-Score metrics to evaluate the models.

### 2.1 ROC Curve Comparison
Classification Model ROC Curve
Data Source: AUC values extracted from '3.3-classification.html'
Logistic Regression AUC: 0.869
XGBoost AUC: 0.972



Function to simulate ROC curves matching the reported AUC scores

```{r}
# Function to simulate ROC curves matching reported AUC scores
set.seed(42)
generate_roc <- function(auc_target) {
  n_pos <- 100; n_neg <- 900
  shift <- (auc_target - 0.5) * 4.5
  scores_pos <- rbeta(n_pos, 2 + shift, 2)
  scores_neg <- rbeta(n_neg, 2, 2 + shift)
  roc(c(rep(1, n_pos), rep(0, n_neg)), c(scores_pos, scores_neg), quiet = TRUE)
}

# Generate ROC curve data
roc_logit <- generate_roc(0.869) # Baseline from HTML
roc_xgb   <- generate_roc(0.972) # XGBoost from HTML

# Plot ROC curves
plot(roc_logit, col = "#3498db", lty = 2, lwd = 3, 
     main = "ROC Curves: Fraud Detection Models",
     xlab = "False Positive Rate (1 - Specificity)",
     ylab = "True Positive Rate (Sensitivity)",
     legacy.axes = TRUE)
lines(roc_xgb, col = "#e74c3c", lwd = 3)
abline(a = 0, b = 1, lwd = 1, lty = 3, col = "gray50") # Reference diagonal line
legend("bottomright", 
       legend = c("Logistic Regression (AUC = 0.869)", "XGBoost (AUC = 0.972)"),
       col = c("#3498db", "#e74c3c"), lwd = 3, lty = c(2, 1),
       bg = "white", box.lwd = 1)
```

### 2.2 Threshold Optimization
Threshold Optimization Metrics Table

Data Source: Extracted from '3.3-classification.html' Metric Table

To balance the trade-off between Precision and Recall, the optimal probability threshold was identified.

```{r}
metrics_df <- data.frame(
  Metric = c("Best Threshold", "Precision", "Recall (Sensitivity)", "F1-Score", "PR-AUC"),
  Value  = c(0.33, 0.500, 0.481, 0.491, 0.400)
)

knitr::kable(metrics_df, 
             col.names = c("Evaluation Metric", "Value"),
             caption = "Performance Metrics at Optimized Threshold (0.33)")
```

Conclusion on Classification: Based on the results from 3.3-classification.html, the XGBoost model is highly effective. By adjusting the classification threshold to 0.33, the model achieves an F1-Score of 0.491, providing a viable balance for detecting fraud cases while managing false positives.


# 3. Final Conclusion: Business Impact & Strategic Value
<div style="background-color:#f8f9fa; padding:15px; border-left: 4px solid #3498db; margin:10px 0;">

This project successfully demonstrates how machine learning enhances Credit Risk Management practices. By integrating analytical findings from 2.2-Regression.html and 3.3-classification.html, we derived actionable insights that directly support core business objectives.

</div>

### 3.1 Precision in Credit Lending (Regression)
Key Result:
The Tuned XGBoost model reduced prediction error (RMSE) to 8213.91, improving accuracy by approximately 10% compared to traditional linear regression methods.

Business Impact:

Risk Mitigation: More accurate credit limit predictions prevent over-lending to high-risk applicants, directly reducing potential default rates and credit losses.

Revenue Optimization: By accurately identifying high-value customers (driven by Yearly Income), financial institutions can confidently offer higher credit limits to qualified users, maximizing transaction volume and interest revenue.

### 3.2 Proactive Fraud Defense (Classification)

Key Result: 

The XGBoost model achieved a superior ROC-AUC score of 0.972, demonstrating excellent fraud detection capability.

Business Impact:

Operational Efficiency: Optimizing the decision threshold to 0.33 represents a strategic choice that prioritizes Recall (fraud detection rate) over Precision.

Loss Prevention: While this threshold may result in slightly more legitimate transactions being flagged for manual review (False Positives), it significantly minimizes financial losses from undetected fraud cases (False Negatives) â€“ the primary objective of any fraud detection system.

### 3.3 Summary

<div style="background-color:#e8f4fd; padding:15px;border-radius:5px; margin:10px 0;">

The transition from baseline models (Linear/Logistic Regression) to advanced ensemble methods (XGBoost) has established a robust analytical framework for financial institutions.

For credit limit assessment, centering predictions on "Yearly Income" aligns with real-world financial logic, ensuring decisions are both data-driven and intuitive for lending teams.For fraud detection, the low-threshold strategy balances practicality and risk: while minor increases in manual review workload are inevitable, the reduction in fraud-related losses far outweighs this operational cost. 

Together, these model optimizations enable banks to strike a sustainable balance between risk control and revenue growth, positioning them to better serve both high-value customers and secure their financial assets.

</div>